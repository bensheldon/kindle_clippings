<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title></title>
  <link rel="stylesheet" href="/kindle_clippings/assets/bootstrap/bootstrap.min.css">
  <script src="/kindle_clippings/assets/bootstrap/bootstrap.bundle.min.js"></script>

  <!-- Don't list this on search engines -->
  <meta name="robots" content="noindex,follow" />
</head>

<body>
  <nav class="navbar navbar-expand-lg navbar-light bg-light sticky-top">
    <div class="container">
    <a class="navbar-brand" href="/kindle_clippings/">Kindle Clippings</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="/kindle_clippings/">Highlights</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/kindle_clippings/books">Books</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/kindle_clippings/authors">Authors</a>
        </li>
      </ul>
    </div>
    </div>
  </nav>

  <div class="container">
  
<h1>Technically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech</h1>

<p>That’s why we all need to pay a lot closer attention to the minutiae we encounter online—the form fields and menus we tend to gloss over so quickly. Because if we want tech companies to be more accountable, we need to be able to identify and articulate what’s going wrong, and put pressure on them to change (or on government to regulate their actions). It’s never been more important that we demand this kind of accountability. Failing to design systems that reflect and represent diverse groups can alienate customers and make people feel marginalized on an individual level, and that would be reason enough for us to demand better. But there’s also a pressing societal concern here. When systems don’t allow users to express their identities, companies end up with data that doesn’t reflect the reality of their users. And as we’ll see in the coming chapters, when companies (and, increasingly, their artificial-intelligence systems) rely on that information to make choices about how their products work, they can wreak havoc—affecting everything from personal safety to political contests to prison sentences. <strong>— 65: 960-968</strong></p>



  </div>
</body>
</html>
